---
title: "PEP_DataMgmtPlan"
format: html
editor: visual
---

# PEP Data Management Plan

## Objective

To outline and implement consistent information management procedures for PEP data.

## Overview

Data are our legacy. The data we collect, manage and analyze today will continue to be available and used by staff in the future. It is, therefore, important to maintain, organize and share our data in a way that will be useful and meaningful to current and future users. This includes:

-   Keeping files organized and up-to-date;

-   Documenting information about the files and the process(es) for creating the files;

-   Cleaning out files (and paperwork) that are not important for the future legacy of the data (e.g. intermediate files, outdated information); and

-   Sharing data with the public and interested partners.

### Foundations to Data Management

A recently published paper in PLOS ONE, [Good Enough Practices in Scientific Computing](https://arxiv.org/pdf/1609.00037.pdf), provides a thorough overview of best practices and workflows for managing scientific data. While much of the focus is maybe out of our scope and focuses more on scientific computing, a few key principles in the data management section are worth focusing on:

1.  **Create the data you wish to see in the world**. The original, raw data collected in the field or offloaded from sensors is rarely the data type and quality of data we would like to share with the world and have our names associated with. Data formats, data organization, column names, value data types and formats (e.g. date-time) can all be transformed and improved into higher quality forms. The better formed the data are, the easier subsequent analyses will be, the more reproducible our science will be, and the more likely it is that others will find use in our data.

2.  **Create analysis-friendly (e.g. tidy) data**. In many cases, analysis-friendly data will be equivalent to the data you wish to see in the world. The key principle at play here is that of "tidy" data. Hadley Wickham's [2014 manuscript](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) does an excellent job outlining the ideal structure of tidy data. The key components of tidy data are:

    a.  Each variable forms a column.

    b.  Each observation forms a row.

    c.  Each type of observational unit forms a table.

3.  Not only are these principles important for analyzing data in programming languages (such as R or Python), they are also key components to a well-organized database. Organizing data in this way will allow easier ingestion into a database; once your data are in a central database, the more tools for exploring and analyzing your data will be available.

4.  **Record all the steps used to process data**. Embracing scripts developed in a programming language (such as R or Python) is essential to providing a robust, reproducible workflow for your data. With large, complex datasets, manually processing data via mouse clicks and Excel or Google Sheets centric workflows are often time-consuming and difficult to reproduce. As part of the planning process for the PEP data management workflow, we will evaluate existing data management processes and identify ways to simplify and automate existing or new steps.

## Data Management Workflow

The figure below describes the general process that PEP will implement for managing data, start to finish. We will ultimately create a Google Drive spreadsheet that inventories the status of each project related to each of the steps in the process and associated details about the data (e.g. where it is stored, where it is shared).

\[INSERT FIGURE HERE\]

\*\*For projects that have already started or are just getting started, we will take steps to update data management and products as outlined in this document.

### Planning and Design

The [NOAA Data Management Planning Procedural Directive](https://nosc.noaa.gov/EDMC/PD.DMP.php) (PD) ([direct link](https://nosc.noaa.gov/EDMC/documents/EDMC-PD-DMP-2.0.1.pdf) to PDF of current version 2.0.1) requires a data management plan to be developed for all environmental data collected by NOAA programs or systems. The PD provides a generic template and guideline for developing a data management plan and a [data management plan repository](https://drive.google.com/drive/u/0/folders/0Bwl6f-PNVtnndG0yOUJ5cGU0RGs) has been established. A common tool for developing data management plans (<https://dmptool.org>) has a template for NOAA. These represent a minimum requirement and, in fact, PEP data management plans will likely be more thorough. All data management plans should be developed in collaboration with the PEP data science lead (Stacie Koslovsky) and established prior to data collection.

Prior to any new data collection, work with S. Koslovsky to:

-   Complete and submit a data management plan;

-   Identify where data will be stored (both short- and long-term; and set up the workspace), what data products will be needed, and what data processing will be required;

-   Create storage locations on network for final data and on Google Drive/elsewhere for intermediate processing; ensure necessary staff have access to these locations;

-   Develop/evaluate/implement data collection strategies that facilitate and simplify data management steps that follow;

-   Coordinate (or at least communicate) with the AFSC OFIS team regarding network storage space needs and any other concerns;

-   Decide how best to organize the metadata for the project (parent vs. child, new vs. add to existing). All metadata entries within InPort will be published publicly and, eventually, used to create standards compliant alongside any open data products. A one-to-one relationship between metadata records and eventual open data products often makes this process easier.

### Data Management

After field work is completed, our aim will be to download, process and QA/QC data as soon as possible. All data should be archived to the network no later than **one month** after the completion of the project.

#### Field Data Collection

During field data collection, begin preliminary data management, as able:

-   Review datasheets for data recording errors;

-   Scan datasheets for backup;

-   Use temporary storage on external/portable drives, cloud providers, or individual government laptops to backup images, scanned datasheets and other critical files; and

-   Enter field data into cloud-based data entry tools.

#### Raw and Original Data

The AFSC's network will be the primary storage location for all environmental data collected by PEP (see more details in Section 5: PEP Network Folder Structure).

-   In practice, the transfer of data from field collection storage to the network should be the first priority upon returning from the field. Temporary storage of files on external/portable drives, cloud providers, individual government laptops should not exceed 30 days after return from the field

-   After data are transferred to the network, files and associated information should be reviewed for consistency.

-   In general, 'raw' or 'original' data collected in the field should not be altered.

-   In cases where the data are known to be incorrect or a more correct value is known, those files or entries should be edited.

-   A common occurrence is for files to not be named properly in the field. File names should be corrected at this time.

-   Additionally, data transcription errors can occur, and this is the appropriate time to fix those errors.

#### Processed and Final Data

After raw/original data are archived to the network, data should be processed and reviewed for outliers. The processing and review steps will vary by project, but generally:

-   We will use automated processes for streamlining and documenting processes whenever possible.

-   Custom data-entry forms and/or spatial processing templates will be created for data entry and processing when needed.

-   Queries and/or other data visualizations will be used to facilitate data QA/QC.

-   Spatial data (grid and other products) will be stored in their native projection in the database.

    -   For many projects, this will be WGS-84.

    -   Spatial data from outside sources (e.g. environmental data) will be stored in the original projection. This may require data to be reprojected for specific analyses or other needs.

-   Environmental data will be updated in the pep DB annually in August. Additional environmental data can be accessed from other data sources online. If you need help with this, contact E. Richmond or S. Koslovsky.

#### Backup Procedures

All data copied to the network are backed up offsite in one of two ways: snap-mirrored to another NMFS facility, tape backup delivered off site (reserved for large files that change infrequently (e.g. imagery, acoustic files, video). In addition, any data that are snap-mirrored are also backed up differentially and that allows incremental restoration (daily for 7 days, weekly for 6 months).

For many small datasets, storing the data on the AFSC network will suffice. For data that are larger or are the result of significant time, effort, or money, a more robust archival system is desirable. For NOAA environmental data, the [National Centers for Environmental Information](https://www.ncei.noaa.gov/) (NCEI) provides this capability. The data submission process is handled through the [Send2NCEI](https://www.nodc.noaa.gov/s2n/) (aka S2N) application. Very large datasets (100s of gigabytes to terabytes) will require additional coordination with NCEI data liaisons (NODC.DataOfficer\@noaa.gov).

### Analysis and Interpretation

Part of the data workflow is to ensure the final products are easily usable and accessible for analyses. Accessing data for each analysis will be different; work with S. Koslovsky to identify the most efficient way to extract data from the DB and/or network for your needs.

Further, we want to emphasize the feedback loop from analytical processes to data management. Our goal with data management is to streamline processing and extraction of information for analyses. This may be updates/improvements to data management processes over time. Feedback and communication are important for ensuring data products meet current and future analytical needs.

### Products

Data are considered final when a project is completed (e.g., CHESS) or when the annual data processing for a project (e.g., harbor seal surveys) is completed. After data are processed and considered final, S. Koslovsky will notify PI and data sharing staff to prepare final datasets for archive and distribution (when appropriate).

#### Overall Workflow

\[INSERT FIGURE HERE\]

#### Metadata

NOAA Fisheries requires that all data collected be documented within the official NOAA Fisheries metadata repository, [InPort](https://inport.nmfs.noaa.gov/inport). InPort provides an extensive suite of tools for editing and managing metadata. Consult with appropriate staff (TBD) to establish a metadata plan and to get started with InPort. However, responsibility for creating, editing, and maintaining InPort records lies with the project leads. Because these records will be available to the public and, in many cases, represent the authoritative documentation of the data set, project leads should devote an appropriate amount of time and thought to development of metadata records.

\[ADD NEW STEPS FOR WORKING WITH CYNTHIA TO WRITE NEW METADATA RECORDS\]

#### Online Repositories

Much of the data collected and processed as part of PEP research activities is intended for public release, either in compliance with NOAA policies (e.g. Public Access to Research Results \[PARR\]) or in support of best practices related to open science and reproducible research. Keeping track of the evolving policies and expanding tools/repositories available can be challenging. Here, we outline our plan for the use of available repositories.

If you are publishing a manuscript and the journal requires the data to be provided on an open data portal, work with S. Koslovsky to identify the most appropriate repository and to ensure metadata are created.

## R Package: pepDataConnect

``` R
# Getting started with pepDataConnect
remotes::install_github('staciekozhardy/pepDataConnect')

# To connect to the database, create a connection
con <- pepDataConnect::pep_connect()

# To load data into your R workspace, use one of the table functions
data <- pepDataConnect::surv_jobss.tbl_detections_processed_ir(con)
```
