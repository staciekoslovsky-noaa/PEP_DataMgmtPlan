[
  {
    "objectID": "PEP_DataMgmtPlan.html",
    "href": "PEP_DataMgmtPlan.html",
    "title": "PEP Data Management Plan",
    "section": "",
    "text": "To outline and implement consistent information management procedures for PEP data."
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#foundations-to-data-management",
    "href": "PEP_DataMgmtPlan.html#foundations-to-data-management",
    "title": "PEP Data Management Plan",
    "section": "Foundations to Data Management",
    "text": "Foundations to Data Management\nA recently published paper in PLOS ONE, Good Enough Practices in Scientific Computing, provides a thorough overview of best practices and workflows for managing scientific data. While much of the focus is maybe out of our scope and focuses more on scientific computing, a few key principles in the data management section are worth focusing on:\n\nCreate the data you wish to see in the world. The original, raw data collected in the field or offloaded from sensors is rarely the data type and quality of data we would like to share with the world and have our names associated with. Data formats, data organization, column names, value data types and formats (e.g. date-time) can all be transformed and improved into higher quality forms. The better formed the data are, the easier subsequent analyses will be, the more reproducible our science will be, and the more likely it is that others will find use in our data.\nCreate analysis-friendly (e.g. tidy) data. In many cases, analysis-friendly data will be equivalent to the data you wish to see in the world. The key principle at play here is that of “tidy” data. Hadley Wickham’s 2014 manuscript does an excellent job outlining the ideal structure of tidy data. The key components of tidy data are:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nNot only are these principles important for analyzing data in programming languages (such as R or Python), they are also key components to a well-organized database. Organizing data in this way will allow easier ingestion into a database; once your data are in a central database, the more tools for exploring and analyzing your data will be available.\nRecord all the steps used to process data. Embracing scripts developed in a programming language (such as R or Python) is essential to providing a robust, reproducible workflow for your data. With large, complex datasets, manually processing data via mouse clicks and Excel or Google Sheets centric workflows are often time-consuming and difficult to reproduce. As part of the planning process for the PEP data management workflow, we will evaluate existing data management processes and identify ways to simplify and automate existing or new steps."
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#data-management-workflow",
    "href": "PEP_DataMgmtPlan.html#data-management-workflow",
    "title": "PEP Data Management Plan",
    "section": "Data Management Workflow",
    "text": "Data Management Workflow\nThe figure below describes the general process that PEP will implement for managing data, start to finish. We will ultimately create a Google Drive spreadsheet that inventories the status of each project related to each of the steps in the process and associated details about the data (e.g. where it is stored, where it is shared).\n[INSERT FIGURE HERE]\n**For projects that have already started or are just getting started, we will take steps to update data management and products as outlined in this document."
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#planning-and-design",
    "href": "PEP_DataMgmtPlan.html#planning-and-design",
    "title": "PEP Data Management Plan",
    "section": "Planning and Design",
    "text": "Planning and Design\nThe NOAA Data Management Planning Procedural Directive (PD) (direct link to PDF of current version 2.0.1) requires a data management plan to be developed for all environmental data collected by NOAA programs or systems. The PD provides a generic template and guideline for developing a data management plan and a data management plan repository has been established. A common tool for developing data management plans (https://dmptool.org) has a template for NOAA. These represent a minimum requirement and, in fact, PEP data management plans will likely be more thorough. All data management plans should be developed in collaboration with the PEP data science lead (Stacie Koslovsky) and established prior to data collection.\nPrior to any new data collection, work with S. Koslovsky to:\n\nComplete and submit a data management plan;\nIdentify where data will be stored (both short- and long-term; and set up the workspace), what data products will be needed, and what data processing will be required;\nCreate storage locations on network for final data and on Google Drive/elsewhere for intermediate processing; ensure necessary staff have access to these locations;\nDevelop/evaluate/implement data collection strategies that facilitate and simplify data management steps that follow;\nCoordinate (or at least communicate) with the AFSC OFIS team regarding network storage space needs and any other concerns;\nDecide how best to organize the metadata for the project (parent vs. child, new vs. add to existing). All metadata entries within InPort will be published publicly and, eventually, used to create standards compliant alongside any open data products. A one-to-one relationship between metadata records and eventual open data products often makes this process easier."
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#data-management",
    "href": "PEP_DataMgmtPlan.html#data-management",
    "title": "PEP Data Management Plan",
    "section": "Data Management",
    "text": "Data Management\nAfter field work is completed, our aim will be to download, process and QA/QC data as soon as possible. All data should be archived to the network no later than one month after the completion of the project.\n\nField Data Collection\nDuring field data collection, begin preliminary data management, as able:\n\nReview datasheets for data recording errors;\nScan datasheets for backup;\nUse temporary storage on external/portable drives, cloud providers, or individual government laptops to backup images, scanned datasheets and other critical files; and\nEnter field data into cloud-based data entry tools.\n\n\n\nRaw and Original Data\nThe AFSC’s network will be the primary storage location for all environmental data collected by PEP (see more details in Section 5: PEP Network Folder Structure).\n\nIn practice, the transfer of data from field collection storage to the network should be the first priority upon returning from the field. Temporary storage of files on external/portable drives, cloud providers, individual government laptops should not exceed 30 days after return from the field\nAfter data are transferred to the network, files and associated information should be reviewed for consistency.\nIn general, ‘raw’ or ‘original’ data collected in the field should not be altered.\nIn cases where the data are known to be incorrect or a more correct value is known, those files or entries should be edited.\nA common occurrence is for files to not be named properly in the field. File names should be corrected at this time.\nAdditionally, data transcription errors can occur, and this is the appropriate time to fix those errors.\n\n\n\nProcessed and Final Data\nAfter raw/original data are archived to the network, data should be processed and reviewed for outliers. The processing and review steps will vary by project, but generally:\n\nWe will use automated processes for streamlining and documenting processes whenever possible.\nCustom data-entry forms and/or spatial processing templates will be created for data entry and processing when needed.\nQueries and/or other data visualizations will be used to facilitate data QA/QC.\nSpatial data (grid and other products) will be stored in their native projection in the database.\n\nFor many projects, this will be WGS-84.\nSpatial data from outside sources (e.g. environmental data) will be stored in the original projection. This may require data to be reprojected for specific analyses or other needs.\n\nEnvironmental data will be updated in the pep DB annually in August. Additional environmental data can be accessed from other data sources online. If you need help with this, contact E. Richmond or S. Koslovsky.\n\n\n\nBackup Procedures\nAll data copied to the network are backed up offsite in one of two ways: snap-mirrored to another NMFS facility, tape backup delivered off site (reserved for large files that change infrequently (e.g. imagery, acoustic files, video). In addition, any data that are snap-mirrored are also backed up differentially and that allows incremental restoration (daily for 7 days, weekly for 6 months).\nFor many small datasets, storing the data on the AFSC network will suffice. For data that are larger or are the result of significant time, effort, or money, a more robust archival system is desirable. For NOAA environmental data, the National Centers for Environmental Information (NCEI) provides this capability. The data submission process is handled through the Send2NCEI (aka S2N) application. Very large datasets (100s of gigabytes to terabytes) will require additional coordination with NCEI data liaisons (NODC.DataOfficer@noaa.gov)."
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#analysis-and-interpretation",
    "href": "PEP_DataMgmtPlan.html#analysis-and-interpretation",
    "title": "PEP Data Management Plan",
    "section": "Analysis and Interpretation",
    "text": "Analysis and Interpretation\nPart of the data workflow is to ensure the final products are easily usable and accessible for analyses. Accessing data for each analysis will be different; work with S. Koslovsky to identify the most efficient way to extract data from the DB and/or network for your needs.\nFurther, we want to emphasize the feedback loop from analytical processes to data management. Our goal with data management is to streamline processing and extraction of information for analyses. This may be updates/improvements to data management processes over time. Feedback and communication are important for ensuring data products meet current and future analytical needs."
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#products",
    "href": "PEP_DataMgmtPlan.html#products",
    "title": "PEP Data Management Plan",
    "section": "Products",
    "text": "Products\nData are considered final when a project is completed (e.g., CHESS) or when the annual data processing for a project (e.g., harbor seal surveys) is completed. After data are processed and considered final, S. Koslovsky will notify PI and data sharing staff to prepare final datasets for archive and distribution (when appropriate).\n\nOverall Workflow\n[INSERT FIGURE HERE]\n\n\nMetadata\nNOAA Fisheries requires that all data collected be documented within the official NOAA Fisheries metadata repository, InPort. InPort provides an extensive suite of tools for editing and managing metadata. Consult with appropriate staff (TBD) to establish a metadata plan and to get started with InPort. However, responsibility for creating, editing, and maintaining InPort records lies with the project leads. Because these records will be available to the public and, in many cases, represent the authoritative documentation of the data set, project leads should devote an appropriate amount of time and thought to development of metadata records.\n[ADD NEW STEPS FOR WORKING WITH CYNTHIA TO WRITE NEW METADATA RECORDS]\n\n\nOnline Repositories\nMuch of the data collected and processed as part of PEP research activities is intended for public release, either in compliance with NOAA policies (e.g. Public Access to Research Results [PARR]) or in support of best practices related to open science and reproducible research. Keeping track of the evolving policies and expanding tools/repositories available can be challenging. Here, we outline our plan for the use of available repositories.\nIf you are publishing a manuscript and the journal requires the data to be provided on an open data portal, work with S. Koslovsky to identify the most appropriate repository and to ensure metadata are created."
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#familiarity-with-network-spaces",
    "href": "PEP_DataMgmtPlan.html#familiarity-with-network-spaces",
    "title": "PEP Data Management Plan",
    "section": "Familiarity with Network Spaces",
    "text": "Familiarity with Network Spaces\nPolar Polar_Imagery Polar_Imagery_2 Polar_Imagery_3"
  },
  {
    "objectID": "PEP_DataMgmtPlan.html#minimum-computer-requirements",
    "href": "PEP_DataMgmtPlan.html#minimum-computer-requirements",
    "title": "PEP Data Management Plan",
    "section": "Minimum Computer Requirements",
    "text": "Minimum Computer Requirements\nBelow is a list of software required for PEP computers.\n\nRequired software for PEP computers.\n\n\n\n\n\n\nProgram\nJustification\n\n\n\n\nQGIS\nFor the most direct connection to PEP database for spatial data\n\n\nArcGIS Pro\nSome preference for Arc version?\n\n\n64-bit PostgreSQL ODBC driver\nFor connecting to PEP database via Microsoft Access\n\n\nMicrosoft Access 2016\nFor using PEP database front-ends (this is part of the Microsoft Suite, so might already be installed by default)\n\n\nEndNote\nFor accessing the PEP library\n\n\nACDSee\nFor image management (field photos and data); we also have licenses for Lightroom\n\n\nVLC Media Player\nFor video management\n\n\nAdobe Acrobat\nFor managing data and forms stored in pdf\n\n\n\nBelow is a list of software recommended for PEP computers.\n\nRecommended software for PEP computers.\n\n\n\n\n\n\nProgram\nJustification\n\n\n\n\nAnaconda(or miniconda) with Python>=3.6\nLots of our tools require Python\n\n\nR\nLots of our tools require R (and kept updated)\n\n\nRStudio (no more than one release behind)\nA user-friendly GUI for R"
  }
]