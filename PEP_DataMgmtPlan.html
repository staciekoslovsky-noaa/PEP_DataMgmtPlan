<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>PEP Data Management Plan</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="PEP_DataMgmtPlan_files/libs/clipboard/clipboard.min.js"></script>
<script src="PEP_DataMgmtPlan_files/libs/quarto-html/quarto.js"></script>
<script src="PEP_DataMgmtPlan_files/libs/quarto-html/popper.min.js"></script>
<script src="PEP_DataMgmtPlan_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="PEP_DataMgmtPlan_files/libs/quarto-html/anchor.min.js"></script>
<link href="PEP_DataMgmtPlan_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="PEP_DataMgmtPlan_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="PEP_DataMgmtPlan_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="PEP_DataMgmtPlan_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="PEP_DataMgmtPlan_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#objective" id="toc-objective" class="nav-link active" data-scroll-target="#objective">Objective</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a>
  <ul class="collapse">
  <li><a href="#foundations-to-data-management" id="toc-foundations-to-data-management" class="nav-link" data-scroll-target="#foundations-to-data-management">Foundations to Data Management</a></li>
  <li><a href="#data-management-workflow" id="toc-data-management-workflow" class="nav-link" data-scroll-target="#data-management-workflow">Data Management Workflow</a>
  <ul class="collapse">
  <li><a href="#plan" id="toc-plan" class="nav-link" data-scroll-target="#plan">Plan</a></li>
  <li><a href="#integrate" id="toc-integrate" class="nav-link" data-scroll-target="#integrate">Integrate</a></li>
  <li><a href="#collect" id="toc-collect" class="nav-link" data-scroll-target="#collect">Collect</a></li>
  <li><a href="#process" id="toc-process" class="nav-link" data-scroll-target="#process">Process</a></li>
  <li><a href="#debrief" id="toc-debrief" class="nav-link" data-scroll-target="#debrief">Debrief</a></li>
  <li><a href="#analyze" id="toc-analyze" class="nav-link" data-scroll-target="#analyze">Analyze</a></li>
  <li><a href="#evaluate" id="toc-evaluate" class="nav-link" data-scroll-target="#evaluate">Evaluate</a></li>
  <li><a href="#share" id="toc-share" class="nav-link" data-scroll-target="#share">Share</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#data-management-on-boarding" id="toc-data-management-on-boarding" class="nav-link" data-scroll-target="#data-management-on-boarding">Data Management On-boarding</a>
  <ul class="collapse">
  <li><a href="#minimum-computer-requirements" id="toc-minimum-computer-requirements" class="nav-link" data-scroll-target="#minimum-computer-requirements">Minimum Computer Requirements</a></li>
  <li><a href="#data-storage" id="toc-data-storage" class="nav-link" data-scroll-target="#data-storage">Data Storage</a>
  <ul class="collapse">
  <li><a href="#individual-storage-resources" id="toc-individual-storage-resources" class="nav-link" data-scroll-target="#individual-storage-resources">Individual Storage Resources</a></li>
  <li><a href="#shared-storage-resources" id="toc-shared-storage-resources" class="nav-link" data-scroll-target="#shared-storage-resources">Shared Storage Resources</a></li>
  <li><a href="#pep-postgresql-database" id="toc-pep-postgresql-database" class="nav-link" data-scroll-target="#pep-postgresql-database">PEP PostgreSQL Database</a></li>
  <li><a href="#pepdataconnect-r-package" id="toc-pepdataconnect-r-package" class="nav-link" data-scroll-target="#pepdataconnect-r-package">pepDataConnect (R package)</a></li>
  </ul></li>
  <li><a href="#frequently-used-program-wide-tools" id="toc-frequently-used-program-wide-tools" class="nav-link" data-scroll-target="#frequently-used-program-wide-tools">Frequently Used Program-wide Tools</a>
  <ul class="collapse">
  <li><a href="#inventory-database" id="toc-inventory-database" class="nav-link" data-scroll-target="#inventory-database">Inventory Database</a></li>
  <li><a href="#extensis-portfolio" id="toc-extensis-portfolio" class="nav-link" data-scroll-target="#extensis-portfolio">Extensis Portfolio</a></li>
  <li><a href="#zotero" id="toc-zotero" class="nav-link" data-scroll-target="#zotero">Zotero</a></li>
  <li><a href="#inport-metadata" id="toc-inport-metadata" class="nav-link" data-scroll-target="#inport-metadata">InPort Metadata</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#data-management-off-boarding" id="toc-data-management-off-boarding" class="nav-link" data-scroll-target="#data-management-off-boarding">Data Management Off-boarding</a></li>
  <li><a href="#data-management-guidelines" id="toc-data-management-guidelines" class="nav-link" data-scroll-target="#data-management-guidelines">Data Management Guidelines</a>
  <ul class="collapse">
  <li><a href="#general-guidelines" id="toc-general-guidelines" class="nav-link" data-scroll-target="#general-guidelines">General Guidelines</a></li>
  <li><a href="#file-naming-conventions" id="toc-file-naming-conventions" class="nav-link" data-scroll-target="#file-naming-conventions">File Naming Conventions</a>
  <ul class="collapse">
  <li><a href="#three-principles-for-file-naming" id="toc-three-principles-for-file-naming" class="nav-link" data-scroll-target="#three-principles-for-file-naming">Three Principles for File Naming</a></li>
  <li><a href="#file-naming-examples" id="toc-file-naming-examples" class="nav-link" data-scroll-target="#file-naming-examples">File Naming Examples</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">PEP Data Management Plan</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This document was last updated on <code>r Sys.Date()</code>.</p>
<section id="objective" class="level1">
<h1>Objective</h1>
<p>Welcome to the Polar Ecosystems Program (PEP; pronounced both as P-E-P and pep) Data Management Plan. The objective of this document is to outline and implement consistent information management procedures for PEP data.</p>
</section>
<section id="overview" class="level1">
<h1>Overview</h1>
<p>Data are our legacy. The data we collect, manage and analyze today will continue to be available and used by staff (and others) in the future. It is, therefore, important to maintain, organize and share our data in a way that will be useful and meaningful to current and future users. This includes:</p>
<ul>
<li><p>Keeping files organized and up-to-date;</p></li>
<li><p>Storing data in machine-readable, human-readable, and tidy formats;</p></li>
<li><p>Documenting information about the files and the process(es) for creating the files;</p></li>
<li><p>Cleaning out files (and paperwork) that are not important for the future legacy of the data (e.g.&nbsp;intermediate files, outdated information); and</p></li>
<li><p>Sharing data with the public and interested partners.</p></li>
</ul>
<section id="foundations-to-data-management" class="level2">
<h2 class="anchored" data-anchor-id="foundations-to-data-management">Foundations to Data Management</h2>
<p>A recently published paper in PLOS ONE, <a href="https://arxiv.org/pdf/1609.00037.pdf">Good Enough Practices in Scientific Computing</a>, provides a thorough overview of best practices and workflows for managing scientific data. While some of the focus is out of our scope, a few key principles in the data management section are worth focusing on:</p>
<ol type="1">
<li><p><strong>Create the data you wish to see in the world</strong>. The original, raw data collected in the field or offloaded from sensors is rarely the data type and quality of data we would like to share with the world and have our names associated with. Data formats, data organization, column names, value data types and formats (e.g.&nbsp;date-time) can all be transformed and improved into higher quality forms. The better formed the data are, the easier subsequent analyses will be, the more reproducible our science will be, and the more likely it is that others will find use in our data.</p></li>
<li><p><strong>Create analysis-friendly (e.g.&nbsp;tidy) data</strong>. In many cases, analysis-friendly data will be equivalent to the data you wish to see in the world. The key principle at play here is that of “tidy” data. Hadley Wickham’s <a href="https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf">2014 manuscript</a> does an excellent job outlining the ideal structure of tidy data.</p>
<ol type="1">
<li><p>The key components of tidy data are:</p>
<ol type="a">
<li><p>Each variable forms a column.</p></li>
<li><p>Each observation forms a row.</p></li>
<li><p>Each type of observational unit forms a table.</p></li>
</ol></li>
<li><p>Not only are these principles important for analyzing data in programming languages (such as R or Python), they are also key components to a well-organized database. Organizing data in this way will allow easier ingestion into a database; once your data are in a central database, the more tools for exploring and analyzing your data will be available.</p></li>
</ol></li>
<li><p><strong>Record all the steps used to process data</strong>. Embracing scripts developed in a programming language (such as R or Python) is essential to providing a robust, reproducible workflow for your data. With large, complex datasets, manually processing data via mouse clicks and spreadsheet-centric workflows are often time-consuming and difficult to reproduce. As part of the planning process for the PEP data management workflow, we will evaluate existing data management processes and identify ways to simplify and automate existing or new steps.</p></li>
</ol>
</section>
<section id="data-management-workflow" class="level2">
<h2 class="anchored" data-anchor-id="data-management-workflow">Data Management Workflow</h2>
<p>The figure below describes the general process that PEP will implement for managing data, start to finish and cyclically for on-going projects. This workflow consists of 7 steps, some of which occur concurrently or similarly timed yet independent of one another.</p>
<p><del>We will ultimately create a Google Drive spreadsheet that inventories the status of each project related to each of the steps in the process and associated details about the data (e.g.&nbsp;where it is stored, where it is shared).</del></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="docs/PEP_DataMgmtPlan_files/images/DataManagementWorkflow.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>**For projects that have already started or are just getting started, we will take steps to update data management and products as outlined in this document.</p>
<section id="plan" class="level3">
<h3 class="anchored" data-anchor-id="plan">Plan</h3>
<p>The <a href="https://nosc.noaa.gov/EDMC/PD.DMP.php">NOAA Data Management Planning Procedural Directive</a> (PD) (<a href="https://nosc.noaa.gov/EDMC/documents/EDMC-PD-DMP-2.0.1.pdf">direct link</a> to PDF of current version 2.0.1) requires a data management plan to be developed for all environmental data collected by NOAA programs or systems. The PD provides a generic template and guideline for developing a data management plan and a <a href="https://drive.google.com/drive/u/0/folders/0Bwl6f-PNVtnndG0yOUJ5cGU0RGs">data management plan repository</a> has been established. A common tool for developing data management plans (<a href="https://dmptool.org" class="uri">https://dmptool.org</a>) has a template for NOAA. These represent a minimum requirement and, in fact, PEP data management plans will likely be more thorough. All data management plans should be developed in collaboration with the PEP data science lead (Stacie Koslovsky) and established prior to data collection.</p>
<p>Prior to any new data collection, work with S. Koslovsky to:</p>
<ul>
<li><p>Complete and submit a data management plan;</p></li>
<li><p>Identify where data will be stored (both short- and long-term; and set up the workspace), what data products will be needed, and what data processing will be required;</p></li>
<li><p>Create storage locations on network for final data and on Google Drive/elsewhere for intermediate processing; ensure necessary staff have access to these locations;</p></li>
<li><p>Develop/evaluate/implement data collection strategies that facilitate and simplify data management steps that follow;</p></li>
<li><p>Coordinate (or at least communicate) with the AFSC OFIS team regarding network storage space needs and any other concerns;</p></li>
<li><p>Decide how best to organize the metadata for the project (parent vs.&nbsp;child, new vs.&nbsp;add to existing). All metadata entries within InPort will be published publicly and, eventually, used to create standards compliant alongside any open data products. A one-to-one relationship between metadata records and eventual open data products often makes this process easier.</p></li>
</ul>
</section>
<section id="integrate" class="level3">
<h3 class="anchored" data-anchor-id="integrate">Integrate</h3>
<p>For on-going projects, we will incorporate feedback from the previous year’s/season’s effort to make improvements to the upcoming effort.</p>
</section>
<section id="collect" class="level3">
<h3 class="anchored" data-anchor-id="collect">Collect</h3>
<section id="field-data-collection" class="level4">
<h4 class="anchored" data-anchor-id="field-data-collection">Field Data Collection</h4>
<p>During field data collection, begin preliminary data management, as able:</p>
<ul>
<li><p>Review datasheets for data recording errors;</p></li>
<li><p>Scan datasheets for backup;</p></li>
<li><p>Use temporary storage on external/portable drives, cloud providers, or individual government laptops to backup images, scanned datasheets and other critical files; and</p></li>
<li><p>Enter field data into cloud-based data entry tools.</p></li>
</ul>
</section>
</section>
<section id="process" class="level3">
<h3 class="anchored" data-anchor-id="process">Process</h3>
<p>After field work is completed, our aim will be to download, process and QA/QC data as soon as possible. All data should be archived to the network no later than <strong>one month</strong> after the completion of the project.</p>
<section id="raw-and-original-data" class="level4">
<h4 class="anchored" data-anchor-id="raw-and-original-data">Raw and Original Data</h4>
<p>The AFSC network is the primary storage location for all data collected by PEP (see more details in PEP Network Folder Structure).</p>
<ul>
<li><p>In practice, the transfer of data from field collection storage to the network should be the first priority upon returning from the field. Temporary storage of files on external/portable drives, cloud providers, individual government laptops should not exceed 30 days after return from the field</p></li>
<li><p>After data are transferred to the network, files and associated information should be reviewed for consistency.</p></li>
<li><p>In general, ‘raw’ or ‘original’ data collected in the field should not be altered.</p></li>
<li><p>In cases where the data are known to be incorrect or a more correct value is known, those files or entries should be edited.</p></li>
<li><p>A common occurrence is for files to not be named properly in the field. File names should be corrected at this time.</p></li>
<li><p>Additionally, data transcription errors can occur, and this is the appropriate time to fix those errors.</p></li>
</ul>
</section>
<section id="processed-and-final-data" class="level4">
<h4 class="anchored" data-anchor-id="processed-and-final-data">Processed and Final Data</h4>
<p>After raw/original data are archived to the network, data should be processed and reviewed for outliers. Data entry and QA/QC should be processed and finalized <strong>within three months</strong> of the field effort, so that other efforts (e.g.&nbsp;image review, counting) are able to proceed in a timely manner.</p>
<p>The processing and review steps will vary by project, but generally:</p>
<ul>
<li><p>We will use automated processes for streamlining and documenting processes whenever possible.</p></li>
<li><p>Custom data-entry forms and/or spatial processing templates will be created for data entry and processing when needed.</p></li>
<li><p>Queries and/or other data visualizations will be used to facilitate data QA/QC.</p></li>
<li><p>Spatial data (grid and other products) will be stored in their native projection in the database.</p>
<ul>
<li><p>For many projects, this will be WGS-84.</p></li>
<li><p>Spatial data from outside sources (e.g.&nbsp;environmental data) will be stored in the original projection. This may require data to be reprojected for specific analyses or other needs.</p></li>
</ul></li>
<li><p>Environmental data will be updated in the pep DB annually in August. Additional environmental data can be accessed from other data sources online. If you need help with this, contact E. Richmond or S. Koslovsky.</p></li>
</ul>
</section>
<section id="backup-procedures" class="level4">
<h4 class="anchored" data-anchor-id="backup-procedures">Backup Procedures</h4>
<p><del>All data copied to the network are backed up offsite in one of two ways: snap-mirrored to another NMFS facility, tape backup delivered off site (reserved for large files that change infrequently (e.g.&nbsp;imagery, acoustic files, video). In addition, any data that are snap-mirrored are also backed up differentially and that allows incremental restoration (daily for 7 days, weekly for 6 months).</del></p>
<p><del>For many small datasets, storing the data on the AFSC network will suffice. For data that are larger or are the result of significant time, effort, or money, a more robust archival system is desirable. For NOAA environmental data, the <a href="https://www.ncei.noaa.gov/">National Centers for Environmental Information</a> (NCEI) provides this capability. The data submission process is handled through the <a href="https://www.nodc.noaa.gov/s2n/">Send2NCEI</a> (aka S2N) application. Very large datasets (100s of gigabytes to terabytes) will require additional coordination with NCEI data liaisons (NODC.DataOfficer@noaa.gov).</del></p>
</section>
</section>
<section id="debrief" class="level3">
<h3 class="anchored" data-anchor-id="debrief">Debrief</h3>
</section>
<section id="analyze" class="level3">
<h3 class="anchored" data-anchor-id="analyze">Analyze</h3>
<p>Part of the data workflow is to ensure the final products are easily usable and accessible for analyses. Accessing data for each analysis will be different; work with S. Koslovsky to identify the most efficient way to extract data from the DB and/or network for your needs.</p>
</section>
<section id="evaluate" class="level3">
<h3 class="anchored" data-anchor-id="evaluate">Evaluate</h3>
<p>We want to emphasize the feedback loop from analytical processes to data management. Our goal with data management is to streamline processing and extraction of information for analyses. This may be updates/improvements to data management processes over time. Feedback and communication are important for ensuring data products meet current and future analytical needs.</p>
</section>
<section id="share" class="level3">
<h3 class="anchored" data-anchor-id="share">Share</h3>
<p>Data are considered final when a project is completed (e.g., CHESS) or when the annual data processing for a project (e.g., harbor seal surveys) is completed. After data are processed and considered final, S. Koslovsky will notify PI and data sharing staff to prepare final datasets for archive and distribution (when appropriate).</p>
<section id="overall-workflow" class="level4">
<h4 class="anchored" data-anchor-id="overall-workflow">Overall Workflow</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="docs/PEP_DataMgmtPlan_files/images/FinalDataWorkflow.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="metadata" class="level4">
<h4 class="anchored" data-anchor-id="metadata">Metadata</h4>
<p>NOAA Fisheries requires that all data collected be documented within the official NOAA Fisheries metadata repository, <a href="https://inport.nmfs.noaa.gov/inport">InPort</a>. InPort provides an extensive suite of tools for editing and managing metadata. Consult with appropriate staff (TBD) to establish a metadata plan and to get started with InPort. However, responsibility for creating, editing, and maintaining InPort records lies with the project leads. Because these records will be available to the public and, in many cases, represent the authoritative documentation of the data set, project leads should devote an appropriate amount of time and thought to development of metadata records.</p>
<p>[ADD NEW STEPS FOR WORKING WITH CYNTHIA TO WRITE NEW METADATA RECORDS]</p>
</section>
<section id="online-repositories" class="level4">
<h4 class="anchored" data-anchor-id="online-repositories">Online Repositories</h4>
<p>Much of the data collected and processed as part of PEP research activities is intended for public release, either in compliance with NOAA policies (e.g.&nbsp;Public Access to Research Results [PARR]) or in support of best practices related to open science and reproducible research. Keeping track of the evolving policies and expanding tools/repositories available can be challenging. Here, we outline our plan for the use of available repositories.</p>
<p>If you are publishing a manuscript and the journal requires the data to be provided on an open data portal, work with S. Koslovsky to identify the most appropriate repository and to ensure metadata are created.</p>
</section>
</section>
</section>
</section>
<section id="data-management-on-boarding" class="level1">
<h1>Data Management On-boarding</h1>
<section id="minimum-computer-requirements" class="level2">
<h2 class="anchored" data-anchor-id="minimum-computer-requirements">Minimum Computer Requirements</h2>
<p>Below is a list of software required for PEP computers.</p>
<table class="table">
<colgroup>
<col style="width: 21%">
<col style="width: 78%">
</colgroup>
<thead>
<tr class="header">
<th>Program</th>
<th>Justification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>QGIS</td>
<td>For the most direct connection to PEP database for spatial data</td>
</tr>
<tr class="even">
<td>64-bit PostgreSQL ODBC driver</td>
<td>For connecting to PEP database via Microsoft Access</td>
</tr>
<tr class="odd">
<td>Microsoft Access 2016</td>
<td>For using PEP database front-ends (this is part of the Microsoft Suite, so might already be installed by default)</td>
</tr>
<tr class="even">
<td>EndNote</td>
<td>For accessing the PEP library</td>
</tr>
<tr class="odd">
<td>ACDSee</td>
<td>For image management (field photos and data); we also have licenses for Lightroom</td>
</tr>
<tr class="even">
<td>Adobe Acrobat</td>
<td>For managing data and forms stored in pdf</td>
</tr>
</tbody>
</table>
<p>Below is a list of software recommended for PEP computers.</p>
<table class="table">
<colgroup>
<col style="width: 46%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th>Program</th>
<th>Justification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Anaconda(or miniconda) with Python&gt;=3.6</td>
<td>Lots of our tools require Python</td>
</tr>
<tr class="even">
<td>R</td>
<td>Lots of our tools require R (and kept updated)</td>
</tr>
<tr class="odd">
<td>RStudio</td>
<td>A user-friendly GUI for R</td>
</tr>
<tr class="even">
<td>ArcGIS Pro</td>
<td>Spatial processing and analysis</td>
</tr>
<tr class="odd">
<td>VLC Media Player</td>
<td>For video management</td>
</tr>
</tbody>
</table>
</section>
<section id="data-storage" class="level2">
<h2 class="anchored" data-anchor-id="data-storage">Data Storage</h2>
<section id="individual-storage-resources" class="level3">
<h3 class="anchored" data-anchor-id="individual-storage-resources">Individual Storage Resources</h3>
<p>No data should be stored exclusively in any of these resources.</p>
<section id="laptop" class="level4">
<h4 class="anchored" data-anchor-id="laptop">Laptop</h4>
</section>
<section id="google-drive" class="level4">
<h4 class="anchored" data-anchor-id="google-drive">Google Drive</h4>
</section>
<section id="individual-users-folders-on-lan" class="level4">
<h4 class="anchored" data-anchor-id="individual-users-folders-on-lan">Individual Users folders on LAN</h4>
</section>
</section>
<section id="shared-storage-resources" class="level3">
<h3 class="anchored" data-anchor-id="shared-storage-resources">Shared Storage Resources</h3>
<section id="pep_googledrive" class="level4">
<h4 class="anchored" data-anchor-id="pep_googledrive">PEP_GoogleDrive</h4>
<p>We have a centralized Google Drive (tied to a generic email account) that serves as our program <a href="https://drive.google.com/drive/folders/1XMadJkz4AqgHctLn0U2xnVChSjHxnyjb">Google Drive repository</a>. Google Drive is an excellent location for sharing files among staff for easy off-site access. Google drive, however, should not be treated as a final storage location for data. Files can be stored here in perpetuity and as appropriate, but data should not. As projects and/or collaborations are completed, data should either be deleted (if they are temporary) or archived to the appropriate location on the network.</p>
</section>
<section id="network-aka-lan" class="level4">
<h4 class="anchored" data-anchor-id="network-aka-lan">Network (aka LAN)</h4>
<p>All original data and supplemental information to data collection will be stored on the LAN in the appropriate location (detailed below). If you have questions about where to store data for a project, contact S. Koslovsky.&nbsp;</p>
<ul>
<li><p>For those projects where data are stored on other locations on the network (e.g.&nbsp;CHESS, harbor seal survey images), create shortcuts to these other network locations within each project folder.&nbsp;</p></li>
<li><p>Final files from intermediate locations will need to be moved to the appropriate location(s) on the network when processing is completed OR when the project becomes collaborative and multiple staff will need access. Project folders should contain final data, datasheets, databases, manuals, metadata, scripts, and intermediate files (as needed).</p></li>
<li><p>For folders under Polar\Analytics, Polar\Data, Polar\Projects on the network, this is a suggested folder structure for each project subfolder. If data easily fit into this schema, use it; if not, data should be organized and documented such that anyone in the program can identify what is where. Not all projects will be able to fit this mold, but this structure will help keep data organized across multiple projects.</p>
<ul>
<li><p><strong>\Data</strong> – contains data collected in numerous formats, including databases, spreadsheets, etc.</p></li>
<li><p><strong>\Docs</strong> – contains project proposals, reports, processing manuals, original field data sheets, etc.</p></li>
<li><p><strong>\GIS</strong> – spatial data and programs associated with the project.</p></li>
<li><p><strong>\Manuscripts</strong> – drafts of manuscript and any associated files.</p></li>
<li><p><strong>\Presentations</strong> – any presentations associated with the project.</p></li>
<li><p><strong>\Scripts</strong> – R, Python, etc. script files.</p></li>
<li><p><strong>READ_MEs</strong> – include read me files wherever appropriate to provide additional information on files within folders.</p></li>
</ul></li>
</ul>
<p>We store our files in a variety of locations on the AFSC internal network:</p>
<ul>
<li><p><a href="about:blank">\\nmfs\akc-nmml\NMML_CHESS_Imagery</a> - images and associated from 2016 CHESS survey</p></li>
<li><p><a href="about:blank">\\nmfs\akc-nmml\Polar</a></p>
<ul>
<li><p>\Analytics</p>
<ul>
<li><p>Will store files related to analytical processes.</p></li>
<li><p>Subfolders should be named YYYY_LastNamePI_ProjectTitle.</p></li>
<li><p>The PI is ultimately responsible for ensuring that all files get stored in this location.</p></li>
<li><p>You do not have to store files in this location if it is inconvenient while you are working; files just need to be archived from computers and Google Drive when processing is complete.</p></li>
</ul></li>
<li><p>\Data</p>
<ul>
<li><p>Will store files related to data collection and creating final datasets.</p></li>
<li><p>Each folder should be a project name, but we do not want too many files to clutter up this location such that it becomes hard to find project data. Folders are likely to rarely be added the root directory.</p></li>
<li><p>Archive datasheets, raw data, etc. should be stored within this directory. When data processing is complete (either for a field season or entirely), files from computers and Google Drive need to be archived to this location.</p></li>
<li><p>\CapturesAndHandling</p>
<ul>
<li>\Data_CruiseUnderway: subfolders should be named YYYY_Location_Species_CruiseName and should match planning folder name under Projects.</li>
</ul></li>
</ul></li>
<li><p>\Legacy</p>
<ul>
<li><p>Will store older project planning and data files that need to be kept for long-term storage, but were not broken out into the new structure.</p></li>
<li><p>Subfolders should be named YYYY-YYYY_ProjectName (as appropriate).</p></li>
</ul></li>
<li><p>\Projects</p>
<ul>
<li><p>Will store files, maps and other information related to field operations (e.g.&nbsp;a folder for coastal harbor seal surveys would include maps for the plane, instructions for the field season, etc.).</p></li>
<li><p>Subfolders should be named in the following format: YYYY_Region_Species_Platform (e.g.&nbsp;2018_BeringSea_IceSeal_DysonCruise).</p></li>
<li><p>After field work is complete, associated files from computers and Google Drive need to be archived to this location.</p></li>
</ul></li>
<li><p>\ProgramMgmt – will store budget, library, permit, etc. information.</p></li>
<li><p>\ResearchProducts</p>
<ul>
<li><p>\Manuscripts: subfolders or pdfs should be named YYYY_Author_Subject.</p></li>
<li><p>\Presentations: final presentations should be named YYYY_Author_Subject.</p></li>
<li><p>\Reports: subfolders or pdfs should be named YYYY_ Subject.</p></li>
<li><p>\WorkshopsAndConferences: subfolders or pdfs should be named YYYY_Author_Subject.</p></li>
</ul></li>
<li><p>\Users</p>
<ul>
<li><p>Subfolder should be named with the staff member’s last name.</p></li>
<li><p>Subfolders should been cleaned out and what needs to be kept moved to _Legacy when staff are no longer employed/involved.</p></li>
</ul></li>
</ul></li>
<li><p><a href="about:blank">\\nmfs\akc-nmml\Polar_Imagery</a></p>
<ul>
<li><p>\Field_Photos – Portfolio monitors this folder for new images</p></li>
<li><p>\Field_Video – Portfolio monitors this folder for new videos</p></li>
<li><p>\Inbox</p></li>
<li><p>\PhotoConsentForms – contains consent forms</p></li>
<li><p>\Portfolio</p></li>
<li><p>\Portfolio_Previews</p></li>
<li><p>\Surveys_HS – contains original and processed images for coastal and glacial harbor seal surveys</p></li>
<li><p>\Surveys_IceSeals – contains original and processed images for ice seal surveys, except CHESS and some BOSS (which can be found on NMML_CHESS_Imagery and Polar_Imagery_2, respectively)</p></li>
<li><p>\Techniques_Test – contains images collected from flights testing equipment and image processing</p></li>
<li><p>\User_Photos – photos taken by staff that do not need to be managed in Portfolio; each staff member has a folder, and they are responsible for managing the photos in this location</p></li>
</ul></li>
<li><p><a href="about:blank">\\nmfs\akc-nmml\Polar_Imagery_2</a></p>
<ul>
<li><p>\Aleutian_HarborSeals_CaptureSiteAnalysis</p></li>
<li><p>\Surveys_IceSeasl_BOSS_2013</p></li>
<li><p>\Surveys_Iliamna_2013</p></li>
<li><p>\Techniques_test</p></li>
<li><p>\xTEMP_BOSS13_Skeyes - can this be deleted?</p></li>
</ul></li>
<li><p><a href="about:blank">\\nmfs\akc-nmml\Polar_Imagery_3</a></p>
<ul>
<li>\jobss_2021</li>
</ul></li>
</ul>
</section>
</section>
<section id="pep-postgresql-database" class="level3">
<h3 class="anchored" data-anchor-id="pep-postgresql-database">PEP PostgreSQL Database</h3>
<section id="overview-1" class="level4">
<h4 class="anchored" data-anchor-id="overview-1">Overview</h4>
<p>The table below details the final data are stored in the pep PostgreSQL database.</p>
<ul>
<li><p>All staff will have read-only access to all data.&nbsp;</p></li>
<li><p>Staff will only have read-write access to the data they are required to edit.</p></li>
<li><p>There are a number of automated scripts that will replicate and/or process data into their final product. This database should be the go-to location data anytime data are needed. Intermediate copies and/or previous exports should always be replaced to ensure you are using the most up-to-date data.</p></li>
<li><p>QGIS is the only software we will use for editing spatial data stored in the DB. It will also be used for importing shapefiles to PostGIS.&nbsp;ArcGIS or other GIS software should only be used for viewing spatial data stored in the DB.</p></li>
</ul>
</section>
<section id="current-schemas" class="level4">
<h4 class="anchored" data-anchor-id="current-schemas">Current Schemas</h4>
<table class="table">
<colgroup>
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 87%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Project</strong></th>
<th><strong>Schema</strong></th>
<th><strong>Status</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Administrative</strong></td>
<td>administrative</td>
<td>Contains data associated with the PEP Dashboard (activity planning, data tracking, etc.). An Access front-end is available for viewing/editing data.</td>
</tr>
<tr class="even">
<td><strong>Annotations</strong></td>
<td>annotations</td>
<td>Contains data associated with application of ML models to imagery; the annotations and imagery data are stored within their project schemas; this schema tracks information related to annotation management. An Access front-end is available for viewing/editing data.</td>
</tr>
<tr class="odd">
<td><strong>Base</strong></td>
<td>base</td>
<td>Contains “THE” grid and environmental covariates extracted to the grid.</td>
</tr>
<tr class="even">
<td><strong>UAS Body Condition</strong></td>
<td>body_condition</td>
<td>Contains image, LRF and measurement data related to the UAS body condition data. An Access front-end will be available for viewing/editing data.</td>
</tr>
<tr class="odd">
<td><strong>Capture</strong></td>
<td>capture</td>
<td>Data have been imported from Excel files, which were exports from the Oracle DB. An Access front-end is available for viewing/editing data.</td>
</tr>
<tr class="even">
<td><strong>Environmental</strong></td>
<td>environ</td>
<td>Includes NARR weather data from 2004-2016 and NSIDC CDR sea ice concentration data*.</td>
</tr>
<tr class="odd">
<td><strong>Inventory</strong></td>
<td>Inventory</td>
<td>Contains data for managing PEP gear inventory and field packlists. An Access front-end is available for viewing/editing data.</td>
</tr>
<tr class="even">
<td><strong>Stock</strong></td>
<td>stock</td>
<td>All data have been migrated from pepgeo to this database.</td>
</tr>
<tr class="odd">
<td><strong>BOSS</strong></td>
<td>surv_boss</td>
<td>All data from Oracle and from the original FMC logs have been ingested into DB.</td>
</tr>
<tr class="even">
<td><strong>ChESS</strong></td>
<td>surv_chess</td>
<td>Data are uploaded to DB from CSV files through R script. All imported data have been QA/QC’ed.&nbsp;A final effort trackline is available.</td>
</tr>
<tr class="odd">
<td><strong>JoBSS</strong></td>
<td>surv_jobss</td>
<td>Image data and annotations from 2021 Beaufort Sea Surveys.</td>
</tr>
<tr class="even">
<td><strong>Ice Seal: Polar Bear</strong></td>
<td>surv_polar_bear</td>
<td>Image data and annotations from 2019 surveys out of Kotzebue and Deadhorse targeting polar bears.</td>
</tr>
<tr class="odd">
<td><p><strong>Harbor Seal:</strong></p>
<p><strong>Coastal Surveys</strong></p></td>
<td>surv_pv_cst</td>
<td>Image data and counts from 2004-2021 are currently QA/QC’ed and available in the DB. ADF&amp;G, NOAA 1996-1997, NOAA 1998-2002 and NOAA 2003 data are available in the DB. Authoritative Iliamna and Pribilof data are also actively managed within this schema. An Access front-end is available for viewing/editing data.</td>
</tr>
<tr class="even">
<td><p><strong>Harbor Seal:</strong></p>
<p><strong>Glacial Surveys</strong></p></td>
<td>surv_pv_gla</td>
<td>Fliight data and counts from surveys are available in the DB. An Access front-end is available for viewing/editing data.</td>
</tr>
<tr class="odd">
<td><strong>Ice Seal: Kotz</strong></td>
<td>surv_test_kotz</td>
<td>Image data and annotations from 2019 testing of in-flight system out of Kotzebue. Fight 01 and R camera views should not be used for any AI/ML.</td>
</tr>
<tr class="even">
<td><strong>Stock</strong></td>
<td>stock</td>
<td>All data have been migrated from older pepgeo DB.</td>
</tr>
<tr class="odd">
<td><strong>Telemetry</strong></td>
<td>telem</td>
<td>All field-related telemetry information has been entered into the DB.</td>
</tr>
</tbody>
</table>
<p>*The sea ice data stored on the pep database are the CDR and sea ice extent products. The original versions of these datasets are also on the network. In addition to these datasets, the bootstrap and nimbus are also stored on the network. The CDR and sea ice extent should be the go-to sea ice data products, which is why they are processed and available in the pep database.</p>
</section>
<section id="naming-conventions" class="level4">
<h4 class="anchored" data-anchor-id="naming-conventions">Naming Conventions</h4>
<p>General Guidelines</p>
<ul>
<li><p>Coordinate with S. Koslovsky before adding new data tables to the database or if you need a stored query added to the database.</p></li>
<li><p>Do not use spaces in database item names. Use underscores between words.</p></li>
<li><p>Data for analyses:</p>
<ul>
<li><p>If you are using data products that are finalized (e.g.&nbsp;CHESS survey) and you need data processed prior to use, the processed data should be stored in a view/query.</p></li>
<li><p>If you are using data products that will continue to be updated (e.g.&nbsp;telemetry data) and you need the processed data preserved, the processed data should either be stored in the database as a separate table or be exported and stored elsewhere (network or computer) with other associated files.</p></li>
</ul></li>
<li><p>For tables</p>
<ul>
<li><p>lku_ - use this prefix for look-up tables.</p></li>
<li><p>geo_ - use this prefix for spatially explicit datasets that are stored in the postgres DB. If you have a table that has spatial data, use the tbl_ prefix. If you have raster, basemap, etc., use the geo_ prefix.</p></li>
<li><p>tbl_ - use this prefix for data tables. These can be spatially enabled.</p></li>
<li><p>res_ - use this prefix for results tables. These are tables that are derived from data stored in tables (tbl_) that need to be stored but are not “raw” data. These should have clear indication of what they are, and if appropriate, a date</p></li>
</ul></li>
<li><p>For views</p>
<ul>
<li><p>qa_ - use this prefix for QA/QC queries.</p></li>
<li><p>res – use this prefix for views that are for analytical purposes.</p></li>
<li><p>Anything else – use logical consistency for any other views stored.</p></li>
</ul></li>
</ul>
</section>
<section id="accessing-data" class="level4">
<h4 class="anchored" data-anchor-id="accessing-data">Accessing Data</h4>
<section id="spatial-data" class="level5">
<h5 class="anchored" data-anchor-id="spatial-data"><strong>Spatial Data</strong></h5>
<p>For viewing spatial data, you can access data from one of these platforms:</p>
<ul>
<li><p>pep_geodatabase</p></li>
<li><p>ArcMap and ArcPro (to connect to the spatial data available on AGOL)</p>
<ul>
<li><p>ArcMap</p>
<ul>
<li><p>Click on the Add Data button, and select “Add Data From ArcGIS Online”</p></li>
<li><p>In the upper right corner of the pop-up window, sign into ArcGIS online using your CAC credentials.</p></li>
<li><p>Once you are logged in, you can add data shared with the PEP SpatialData group under “My Groups”.</p></li>
</ul></li>
<li><p>ArcPro</p>
<ul>
<li><p>You are automatically logged into ArcGIS online when you open ArcPro.&nbsp;</p></li>
<li><p>Once you have created a new project, you can add data from the Portal. Data shared to the PEP Spatial Data group will be accessible from Groups under Portal.</p></li>
</ul></li>
<li><p>ArcGIS Online</p></li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p>Once signed in to ArcGIS online, click on “Groups” at the top of the page</p></li>
<li><p>Click on the group, “PEP Spatial Data” and open the item page for the dataset you want to download</p></li>
<li><p>On the right side of the page, go to “Export Data” and choose the format you want to export</p></li>
<li><p>An export box will appear with a default title for the data export. You can rename this if you’d like. You must enter at least one tag for your dataset to allow for export. Exported items from ArcGIS online are stored in the root folder of “My Content”. If exporting to shapefile, a compressed file (.zip) is created and can be downloaded and saved your computer.</p></li>
</ul>
<!-- -->
<ul>
<li><p>QGIS (to connect to spatial data&nbsp;</p>
<ul>
<li><p>Right click PostGIS, and select “New Connection…”</p></li>
<li><p>In the Create a New PostGIS Connection window, specify the following options; then, click OK.</p>
<ul>
<li><p>Name: pep</p></li>
<li><p>Service: (leave blank)</p></li>
<li><p>Host: 161.55.120.122</p></li>
<li><p>Port: 5432</p></li>
<li><p>Database: pep</p></li>
<li><p>Username: your user name (check box to save)</p></li>
<li><p>Password: your password (check box to save)</p></li>
</ul></li>
<li><p>Once created, you will see a new connection to the database under PostGIS. The available data are listed under each schema, and only spatial data are visible.</p></li>
</ul></li>
</ul>
<p>For entering spatial data, process the data as instructed for each individual project.&nbsp;</p>
</section>
<section id="tabular-data" class="level5">
<h5 class="anchored" data-anchor-id="tabular-data"><strong>Tabular Data</strong></h5>
<p>For viewing tabular data:&nbsp;</p>
<ul>
<li><p>Project-specific Access frontends are available for the following projects:</p>
<ul>
<li><p>Capture (for entering/viewing data entered from datasheets)</p></li>
<li><p>Coastal Harbor Seal Surveys (for track level metadata)</p></li>
</ul></li>
<li><p>You can directly connect to the pep DB using R with the RPostgreSQL package. This connection string will get you started: con &lt;- dbConnect(PostgreSQL(), dbname = “pep”, host = “161.55.120.122”, user = “UserName”, password = “Password”)</p></li>
<li><p>If you need access to or exports of other tabular data from the pep DB, contact S. Hardy.</p></li>
</ul>
</section>
</section>
<section id="backup-procedure" class="level4">
<h4 class="anchored" data-anchor-id="backup-procedure">Backup Procedure</h4>
<p>The pepDB will be backed up according to the following schedule:</p>
<ul>
<li><p>Daily directory backup: The pep DB will be backed up daily to tape. The last 5 days will also be stored on the VM.&nbsp;</p></li>
<li><p>Monthly full VM backup: The virtual machine on which the pep DB resides will be fully backed up to tape monthly. The last 3 months will also be stored on the VM.</p></li>
<li><p>The DB backups will be tested every 6 months (March and September) to ensure they are working properly, and data can be restored.</p></li>
</ul>
</section>
</section>
<section id="pepdataconnect-r-package" class="level3">
<h3 class="anchored" data-anchor-id="pepdataconnect-r-package">pepDataConnect (R package)</h3>
<p>There are of tables (and linkages among them) in PEP database, and this can be overwhelming to learn and can lead to potential issues in how data are linked together or extracted from the DB. The pepDataConnect R package was created to easily connect PEP staff to the database and to ensure the quality of the data being retrieved. The R code snippet below will guide you through the process of installing the R package and an example of code to get a data table from the DB. There is <em>extensive</em> documentation in the</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Getting started with pepDataConnect remotes::install_github('staciekoslovsky-noaa/pepDataConnect')  # To connect to the database, create a connection con &lt;- pepDataConnect::pep_connect()  # To load data into your R workspace, use one of the table functions data &lt;- pepDataConnect::surv_jobss.tbl_detections_processed_ir(con)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Currently, the pepDataConnect R package has been set-up to interact with the database for the following schemas:</p>
<ul>
<li><p>Coastal harbor seal survey data (surv_pv_cst)</p></li>
<li><p>JoBSS ice seal survey (surv_jobss)</p></li>
</ul>
<p>Other schemas will slowly be added to the R package, as prioritized each year</p>
</section>
</section>
<section id="frequently-used-program-wide-tools" class="level2">
<h2 class="anchored" data-anchor-id="frequently-used-program-wide-tools">Frequently Used Program-wide Tools</h2>
<section id="inventory-database" class="level3">
<h3 class="anchored" data-anchor-id="inventory-database">Inventory Database</h3>
</section>
<section id="extensis-portfolio" class="level3">
<h3 class="anchored" data-anchor-id="extensis-portfolio">Extensis Portfolio</h3>
</section>
<section id="zotero" class="level3">
<h3 class="anchored" data-anchor-id="zotero">Zotero</h3>
</section>
<section id="inport-metadata" class="level3">
<h3 class="anchored" data-anchor-id="inport-metadata">InPort Metadata</h3>
</section>
</section>
</section>
<section id="data-management-off-boarding" class="level1">
<h1>Data Management Off-boarding</h1>
<p>(this section to be completed after PEP Data Days 2025)</p>
</section>
<section id="data-management-guidelines" class="level1">
<h1>Data Management Guidelines</h1>
<section id="general-guidelines" class="level2">
<h2 class="anchored" data-anchor-id="general-guidelines">General Guidelines</h2>
<p><strong>Create folders with a top-down hierarchy</strong>, i.e.&nbsp;folders should get more specific as you get deeper into folder organization. This will make long-term organization more efficient, will consolidate files to a layout that will be easy for others to interpret, and will improve information transfer among staff.</p>
<p><strong>Create and maintain metadata documentation for your files</strong>.&nbsp;These can be shorter documents describing the files within a specific folder or longer documents describing the work that went into creating each of the files. The complexity of each file/folder dictates the level of detail that should be documented in the metadata. This level of metadata can be most simply managed using a word document or a text file.</p>
<p><strong>Copy files…do not drag-and-drop</strong>.&nbsp;When you need to move files on the network or your computer, move them by either copying the files to the new location and deleting the files from the old location or cutting and pasting from the old location to the new location (either option instead of drag-and-drop). Use batch processing whenever dealing with large volumes of data (example link to SMK code for this in R).</p>
</section>
<section id="file-naming-conventions" class="level2">
<h2 class="anchored" data-anchor-id="file-naming-conventions">File Naming Conventions</h2>
<p>Name and organize your files in such a way that you know what they mean at 3 am (or in a crunch)!</p>
<section id="three-principles-for-file-naming" class="level3">
<h3 class="anchored" data-anchor-id="three-principles-for-file-naming">Three Principles for File Naming</h3>
<ul>
<li><p><strong>Human readable</strong></p>
<ul>
<li><p>File names are easy to understand by anyone, not just you. ☺</p></li>
<li><p>Using versioning (when appropriate) to track different versions of your files (dates are a great way to do this).</p></li>
<li><p>Keep file names as short as possible.</p></li>
</ul></li>
<li><p><strong>Machine readable</strong></p>
<ul>
<li><p>Avoid spaces, punctuation, accented characters.</p></li>
<li><p>Case-sensitivity!</p></li>
<li><p>Deliberately use of delimiters (e.g._ - .) instead of spaces.</p></li>
</ul></li>
<li><p><strong>Orders logically</strong></p>
<ul>
<li><p>Name files from most general to most specific, so similar items cluster together when sorted by name and are easy to read (e.g.&nbsp;HarborSeal_Locations and HarborSeal_Polygons).</p></li>
<li><p>YYYY-MM-DD sorts better than DD-MMM-YYYY.</p></li>
<li><p>Use leading zeroes! (e.g.&nbsp;01, 02, 10 – instead of 1, 2, 10).</p></li>
</ul></li>
</ul>
</section>
<section id="file-naming-examples" class="level3">
<h3 class="anchored" data-anchor-id="file-naming-examples">File Naming Examples</h3>
<p><strong>BAD</strong></p>
<p>Plan 1.docx</p>
<p>Why won’t this work.R</p>
<p>Asdfasdf.jpg</p>
<p>New.pdf</p>
<p>NewFinal.pdf</p>
<p>NewFinalFinal.pdf</p>
<p><strong>GOOD</strong></p>
<p>DataMgmtPlan_20170825.docx</p>
<p>CHESS_DataImport.R</p>
<p>Figure01.jpg</p>
<p>ActivityPlan_20170815.pdf</p>
<p>ActivityPlan_20170818.pdf</p>
<p>ActivityPlan_20170822.pd</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>